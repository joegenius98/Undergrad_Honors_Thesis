{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms.functional import rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, images):\n",
    "        self.images = images\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.images[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    print(\"Here is the batch in `collate_fn`\")\n",
    "    print(batch)\n",
    "    print(\"List of Tensor sizes in batch:\")\n",
    "    print([tensor.shape for tensor in batch])\n",
    "\n",
    "    # get image dimensions, assuming all images are the same size\n",
    "    # and it is number-of-channels (nc) first\n",
    "    nc, h, w = batch[0].shape[-3], batch[0].shape[-2], batch[0].shape[-1]\n",
    "\n",
    "    batch_size = len(batch)\n",
    "\n",
    "\n",
    "    \"\"\"to return batches of 3 (original image, augmented original, and another image)\n",
    "    implies we need at least two separate images to work off of\"\"\" \n",
    "    if batch_size >= 2:\n",
    "        print(f\"Batch size: {batch_size}\")\n",
    "        # format: (batch_size, 3 for (original, augmented, another), num_channels, height, width)\n",
    "        images_batch = torch.zeros((batch_size, 3, nc, h, w))\n",
    "        \n",
    "        for i in range(batch_size - 1):         \n",
    "            first_image_index = i\n",
    "            second_image_index = i + 1 # can be i and i + 1 if we toggle shuffle = True, so that it's random\n",
    "            first_image = batch[first_image_index]\n",
    "            second_image = batch[second_image_index]\n",
    "            first_image_augmented = rotate(first_image, 180)\n",
    "\n",
    "            images_batch[i, 0, :, :, :] = first_image\n",
    "            images_batch[i, 1, :, :, :] = first_image_augmented\n",
    "            images_batch[i, 2, :, :, :] = second_image\n",
    "        \n",
    "        images_batch[batch_size-1, 0, :, :, :] = batch[batch_size-1]\n",
    "        images_batch[batch_size-1, 1, :, :, :] = rotate(batch[batch_size-1], 180)\n",
    "        images_batch[batch_size-1, 2, :, :, :] = batch[0] \n",
    "\n",
    "\n",
    "    # otherwise, just create an (original image, augmented original) pair\n",
    "    else:\n",
    "        assert batch_size == 1\n",
    "        images_batch = torch.zeros((batch_size, 2, nc, h, w))\n",
    "        images_batch[0, 0, :, :, :] = batch[0]\n",
    "        images_batch[0, 1, :, :, :] = rotate(batch[0], 180)\n",
    "        \n",
    "    return images_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 4)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1, *[3, 4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Data Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the batch in `collate_fn`\n",
      "[tensor([[[ 0.1320, -0.1185, -0.1576],\n",
      "         [-0.5248, -0.3598, -1.1011],\n",
      "         [ 0.2210,  0.0549,  2.9979]]]), tensor([[[-0.4236,  0.1173,  1.3468],\n",
      "         [ 0.2980,  2.0543,  1.9087],\n",
      "         [-0.7844,  0.9213, -0.8448]]])]\n",
      "List of Tensor sizes in batch:\n",
      "[torch.Size([1, 3, 3]), torch.Size([1, 3, 3])]\n",
      "Batch size: 2\n",
      "Batch output w/ shape torch.Size([6, 1, 3, 3])\n",
      "tensor([[[[ 0.1320, -0.1185, -0.1576],\n",
      "          [-0.5248, -0.3598, -1.1011],\n",
      "          [ 0.2210,  0.0549,  2.9979]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9979,  0.0549,  0.2210],\n",
      "          [-1.1011, -0.3598, -0.5248],\n",
      "          [-0.1576, -0.1185,  0.1320]]],\n",
      "\n",
      "\n",
      "        [[[-0.4236,  0.1173,  1.3468],\n",
      "          [ 0.2980,  2.0543,  1.9087],\n",
      "          [-0.7844,  0.9213, -0.8448]]],\n",
      "\n",
      "\n",
      "        [[[-0.4236,  0.1173,  1.3468],\n",
      "          [ 0.2980,  2.0543,  1.9087],\n",
      "          [-0.7844,  0.9213, -0.8448]]],\n",
      "\n",
      "\n",
      "        [[[-0.8448,  0.9213, -0.7844],\n",
      "          [ 1.9087,  2.0543,  0.2980],\n",
      "          [ 1.3468,  0.1173, -0.4236]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1320, -0.1185, -0.1576],\n",
      "          [-0.5248, -0.3598, -1.1011],\n",
      "          [ 0.2210,  0.0549,  2.9979]]]])\n",
      "\n",
      "\n",
      "\n",
      "Here is the batch in `collate_fn`\n",
      "[tensor([[[-0.3744, -0.7016, -0.7399],\n",
      "         [ 0.3381, -0.9089,  1.0014],\n",
      "         [-0.7326,  2.3583,  1.0136]]]), tensor([[[ 0.2606,  0.5209,  0.1146],\n",
      "         [ 1.0323,  0.6189,  1.1993],\n",
      "         [ 1.3810, -0.4900, -1.3228]]])]\n",
      "List of Tensor sizes in batch:\n",
      "[torch.Size([1, 3, 3]), torch.Size([1, 3, 3])]\n",
      "Batch size: 2\n",
      "Batch output w/ shape torch.Size([6, 1, 3, 3])\n",
      "tensor([[[[-0.3744, -0.7016, -0.7399],\n",
      "          [ 0.3381, -0.9089,  1.0014],\n",
      "          [-0.7326,  2.3583,  1.0136]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0136,  2.3583, -0.7326],\n",
      "          [ 1.0014, -0.9089,  0.3381],\n",
      "          [-0.7399, -0.7016, -0.3744]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2606,  0.5209,  0.1146],\n",
      "          [ 1.0323,  0.6189,  1.1993],\n",
      "          [ 1.3810, -0.4900, -1.3228]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2606,  0.5209,  0.1146],\n",
      "          [ 1.0323,  0.6189,  1.1993],\n",
      "          [ 1.3810, -0.4900, -1.3228]]],\n",
      "\n",
      "\n",
      "        [[[-1.3228, -0.4900,  1.3810],\n",
      "          [ 1.1993,  0.6189,  1.0323],\n",
      "          [ 0.1146,  0.5209,  0.2606]]],\n",
      "\n",
      "\n",
      "        [[[-0.3744, -0.7016, -0.7399],\n",
      "          [ 0.3381, -0.9089,  1.0014],\n",
      "          [-0.7326,  2.3583,  1.0136]]]])\n",
      "\n",
      "\n",
      "\n",
      "Here is the batch in `collate_fn`\n",
      "[tensor([[[-1.4746, -0.6602, -0.0673],\n",
      "         [-0.3878,  0.9570,  0.0643],\n",
      "         [-0.8978,  0.9141, -0.3790]]])]\n",
      "List of Tensor sizes in batch:\n",
      "[torch.Size([1, 3, 3])]\n",
      "Batch output w/ shape torch.Size([2, 1, 3, 3])\n",
      "tensor([[[[-1.4746, -0.6602, -0.0673],\n",
      "          [-0.3878,  0.9570,  0.0643],\n",
      "          [-0.8978,  0.9141, -0.3790]]],\n",
      "\n",
      "\n",
      "        [[[-0.3790,  0.9141, -0.8978],\n",
      "          [ 0.0643,  0.9570, -0.3878],\n",
      "          [-0.0673, -0.6602, -1.4746]]]])\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "images = torch.randn(5, 1, 3, 3)\n",
    "dataset = ImageDataset(images)\n",
    "dataloader = DataLoader(dataset, batch_size=2, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "for batch in dataloader:\n",
    "    batch = batch.view(batch.shape[0]*batch.shape[1], *batch.shape[2:]) \n",
    "    print(f\"Batch output w/ shape {batch.shape}\")\n",
    "    print(batch)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 3, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CelebA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/honors_thesis2/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "class CustomImageFolder(ImageFolder):\n",
    "    def __init__(self, root, transform=None):\n",
    "        super(CustomImageFolder, self).__init__(root, transform)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.imgs[index][0]\n",
    "        img = self.loader(path)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(), ])\n",
    "\n",
    "train_kwargs = {'root': \"./data/CelebA\", 'transform': transform}\n",
    "celeba_data = CustomImageFolder(**train_kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3DChairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "honors_thesis2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b983af6535c1fcf545062201351deb290a073ac805dfed11686d510c45d59513"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
